{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer-to-Peer (P2P) with 2x NVIDIA RTX A5000   \n",
    "\n",
    "- cloud provider: [runpod.io](https://www.runpod.io/)\n",
    "- pod: 2 x RTX A5000 (19 vCPU 100 GB RAM)\n",
    "- image: `runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 27 20:14:25 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000               On  | 00000000:56:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              24W / 230W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000               On  | 00000000:57:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              15W / 230W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\n",
      "GPU0\t X \tPIX\tSYS\tSYS\t0-23,48-71\t0\t\tN/A\n",
      "GPU1\tPIX\t X \tSYS\tSYS\t0-23,48-71\t0\t\tN/A\n",
      "NIC0\tSYS\tSYS\t X \tPIX\t\t\t\t\n",
      "NIC1\tSYS\tSYS\tPIX\t X \t\t\t\t\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n",
      "\n",
      "NIC Legend:\n",
      "\n",
      "  NIC0: mlx5_0\n",
      "  NIC1: mlx5_1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building [nccl-tests](https://github.com/NVIDIA/nccl-tests/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make -C src build BUILDDIR=/workspace/code/cuda-mode/p2p-perf/nccl-tests/build\n",
      "make[1]: Entering directory '/workspace/code/cuda-mode/p2p-perf/nccl-tests/src'\n",
      "Compiling  timer.cc                            > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/timer.o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/verifiable/verifiable.o\n",
      "Compiling  all_reduce.cu                       > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/all_reduce.o\n",
      "Compiling  common.cu                           > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/common.o\n",
      "Linking  /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/all_reduce.o > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/all_reduce_perf\n",
      "Compiling  all_gather.cu                       > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/all_gather.o\n",
      "Linking  /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/all_gather.o > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/all_gather_perf\n",
      "Compiling  broadcast.cu                        > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/broadcast.o\n",
      "Linking  /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/broadcast.o > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/broadcast_perf\n",
      "Compiling  reduce_scatter.cu                   > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/reduce_scatter.o\n",
      "Linking  /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/reduce_scatter.o > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/reduce_scatter_perf\n",
      "Compiling  reduce.cu                           > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/reduce.o\n",
      "Linking  /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/reduce.o > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/reduce_perf\n",
      "Compiling  alltoall.cu                         > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/alltoall.o\n",
      "Linking  /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/alltoall.o > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/alltoall_perf\n",
      "Compiling  scatter.cu                          > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/scatter.o\n",
      "Linking  /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/scatter.o > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/scatter_perf\n",
      "Compiling  gather.cu                           > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/gather.o\n",
      "Linking  /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/gather.o > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/gather_perf\n",
      "Compiling  sendrecv.cu                         > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/sendrecv.o\n",
      "Linking  /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/sendrecv.o > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/sendrecv_perf\n",
      "Compiling  hypercube.cu                        > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/hypercube.o\n",
      "Linking  /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/hypercube.o > /workspace/code/cuda-mode/p2p-perf/nccl-tests/build/hypercube_perf\n",
      "make[1]: Leaving directory '/workspace/code/cuda-mode/p2p-perf/nccl-tests/src'\n"
     ]
    }
   ],
   "source": [
    "!cd ../nccl-tests/ && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running all_reduce_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nThread 1 nGpus 2 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0\n",
      "#\n",
      "# Using devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  Rank  0 Group  0 Pid 686484 on 58e5f0f169a6 device  0 [0x56] NVIDIA RTX A5000\n",
      "#  Rank  1 Group  0 Pid 686484 on 58e5f0f169a6 device  1 [0x57] NVIDIA RTX A5000\n",
      "#\n",
      "#                                                              out-of-place                       in-place          \n",
      "#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong\n",
      "#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \n",
      "           8             2     float     sum      -1    18.02    0.00    0.00      0    18.24    0.00    0.00      0\n",
      "          16             4     float     sum      -1    18.96    0.00    0.00      0    16.45    0.00    0.00      0\n",
      "          32             8     float     sum      -1    17.12    0.00    0.00      0    16.92    0.00    0.00      0\n",
      "          64            16     float     sum      -1    15.88    0.00    0.00      0    16.11    0.00    0.00      0\n",
      "         128            32     float     sum      -1    16.00    0.01    0.01      0    14.94    0.01    0.01      0\n",
      "         256            64     float     sum      -1    14.93    0.02    0.02      0    15.42    0.02    0.02      0\n",
      "         512           128     float     sum      -1    15.35    0.03    0.03      0    15.15    0.03    0.03      0\n",
      "        1024           256     float     sum      -1    15.45    0.07    0.07      0    14.66    0.07    0.07      0\n",
      "        2048           512     float     sum      -1    14.51    0.14    0.14      0    14.65    0.14    0.14      0\n",
      "        4096          1024     float     sum      -1    14.43    0.28    0.28      0    13.83    0.30    0.30      0\n",
      "        8192          2048     float     sum      -1    13.88    0.59    0.59      0    14.29    0.57    0.57      0\n",
      "       16384          4096     float     sum      -1    13.58    1.21    1.21      0    13.15    1.25    1.25      0\n",
      "       32768          8192     float     sum      -1    14.22    2.30    2.30      0    13.53    2.42    2.42      0\n",
      "       65536         16384     float     sum      -1    13.59    4.82    4.82      0    13.63    4.81    4.81      0\n",
      "      131072         32768     float     sum      -1    33.23    3.94    3.94      0    32.96    3.98    3.98      0\n",
      "      262144         65536     float     sum      -1    39.62    6.62    6.62      0    39.60    6.62    6.62      0\n",
      "      524288        131072     float     sum      -1    50.57   10.37   10.37      0    50.28   10.43   10.43      0\n",
      "     1048576        262144     float     sum      -1    73.07   14.35   14.35      0    72.62   14.44   14.44      0\n",
      "     2097152        524288     float     sum      -1    116.5   18.00   18.00      0    113.2   18.53   18.53      0\n",
      "     4194304       1048576     float     sum      -1    206.7   20.29   20.29      0    205.9   20.38   20.38      0\n",
      "     8388608       2097152     float     sum      -1    394.2   21.28   21.28      0    392.7   21.36   21.36      0\n",
      "    16777216       4194304     float     sum      -1    772.8   21.71   21.71      0    773.0   21.70   21.70      0\n",
      "    33554432       8388608     float     sum      -1   1532.1   21.90   21.90      0   1531.6   21.91   21.91      0\n",
      "    67108864      16777216     float     sum      -1   3043.4   22.05   22.05      0   3049.8   22.00   22.00      0\n",
      "   134217728      33554432     float     sum      -1   6074.2   22.10   22.10      0   6078.4   22.08   22.08      0\n",
      "# Out of bounds values : 0 OK\n",
      "# Avg bus bandwidth    : 7.70273 \n",
      "#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!../nccl-tests/build/all_reduce_perf -b 8 -e 128M -f 2 -g 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0->cuda:1: 24.572 GB/s\n",
      "cuda:1->cuda:0: 24.535 GB/s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "device0 = torch.device(\"cuda\", 0)\n",
    "device1 = torch.device(\"cuda\", 1)\n",
    "\n",
    "x0 = torch.randn(1024, 1024, 1024, dtype=torch.float32, device=device0)\n",
    "x1 = torch.randint(0,100, (1024, 1024, 1024), dtype=torch.long, device=device1)\n",
    "\n",
    "def copy_tensor(x, dest_device):\n",
    "    y = x.to(dest_device, non_blocking=False, copy=False)\n",
    "    return y\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='copy_tensor(x0, device1)',\n",
    "    setup='from __main__ import copy_tensor',\n",
    "    globals={'x0': x0, 'device1': device1},\n",
    "    num_threads=1)\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='copy_tensor(x1, device1)',\n",
    "    setup='from __main__ import copy_tensor',\n",
    "    globals={'x1': x1, 'device1': device0},\n",
    "    num_threads=1)\n",
    "\n",
    "# sanity check\n",
    "s0 = x0.sum().cpu()\n",
    "y1 = copy_tensor(x0, device1)\n",
    "s1 = y1.sum().cpu()\n",
    "assert torch.abs(s0-s1) < 1e-5\n",
    "\n",
    "m0 = t0.timeit(100)\n",
    "storage_size0 = x0.untyped_storage().size()\n",
    "print(f\"{device0}->{device1}: {storage_size0/m0.mean/2**30:.3f} GB/s\")\n",
    "\n",
    "m1 = t1.timeit(100)\n",
    "storage_size1 = x1.untyped_storage().size()\n",
    "print(f\"{device1}->{device0}: {storage_size1/m1.mean/2**30:.3f} GB/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank=0 send N=50, elapsed_time=8.6825s, 23.035 GB/s, sum=-9481.1240234375 (cuda:0)\n",
      "rank=1 recv N=50, elapsed_time=8.6485s, 23.125 GB/s, sum=-9481.1240234375 (cuda:1)\n",
      "rank=0 broadcast(a, src=0) N=50, elapsed_time=9.5473s, 20.948 GB/s, sum=-9481.1240234375 (cuda:0)\n",
      "rank=0 broadcast(a, src=1) N=50, elapsed_time=9.3507s, 21.389 GB/s, sum=-9481.1240234375 (cuda:0)\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=1 /usr/local/bin/torchrun --standalone --nnodes=1 --nproc-per-node 2 torch_distributed_nccl_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running cuda-samples simpleP2P and p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Nothing to be done for 'all'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[../cuda-samples/Samples/0_Introduction/simpleP2P/simpleP2P] - Starting...\n",
      "Checking for multiple GPUs...\n",
      "CUDA-capable device count: 2\n",
      "\n",
      "Checking GPU(s) for support of peer to peer memory access...\n",
      "> Peer access from NVIDIA RTX A5000 (GPU0) -> NVIDIA RTX A5000 (GPU1) : Yes\n",
      "> Peer access from NVIDIA RTX A5000 (GPU1) -> NVIDIA RTX A5000 (GPU0) : Yes\n",
      "Enabling peer access between GPU0 and GPU1...\n",
      "Allocating buffers (64MB on GPU0, GPU1 and CPU Host)...\n",
      "Creating event handles...\n",
      "cudaMemcpyPeer / cudaMemcpy between GPU0 and GPU1: 24.47GB/s\n",
      "Preparing host buffer and memcpy to GPU0...\n",
      "Run kernel on GPU1, taking source data from GPU0 and writing to GPU1...\n",
      "Run kernel on GPU0, taking source data from GPU1 and writing to GPU0...\n",
      "Copy data back to host from GPU0 and verify results...\n",
      "Disabling peer access...\n",
      "Shutting down...\n",
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "!cd ../cuda-samples/Samples/0_Introduction/simpleP2P/ && make\n",
    "!../cuda-samples/Samples/0_Introduction/simpleP2P/simpleP2P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o p2pBandwidthLatencyTest.o -c p2pBandwidthLatencyTest.cu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o p2pBandwidthLatencyTest p2pBandwidthLatencyTest.o \n",
      "mkdir -p ../../../bin/x86_64/linux/release\n",
      "cp p2pBandwidthLatencyTest ../../../bin/x86_64/linux/release\n",
      "[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\n",
      "Device: 0, NVIDIA RTX A5000, pciBusID: 56, pciDeviceID: 0, pciDomainID:0\n",
      "Device: 1, NVIDIA RTX A5000, pciBusID: 57, pciDeviceID: 0, pciDomainID:0\n",
      "Device=0 CAN Access Peer Device=1\n",
      "Device=1 CAN Access Peer Device=0\n",
      "\n",
      "***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\n",
      "So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\n",
      "\n",
      "P2P Connectivity Matrix\n",
      "     D\\D     0     1\n",
      "     0\t     1     1\n",
      "     1\t     1     1\n",
      "Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1 \n",
      "     0 674.07  18.18 \n",
      "     1  18.21 676.11 \n",
      "Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n",
      "   D\\D     0      1 \n",
      "     0 676.70  26.39 \n",
      "     1  26.39 676.11 \n",
      "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1 \n",
      "     0 680.68  19.37 \n",
      "     1  19.50 681.87 \n",
      "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1 \n",
      "     0 680.98  52.18 \n",
      "     1  52.18 680.38 \n",
      "P2P=Disabled Latency Matrix (us)\n",
      "   GPU     0      1 \n",
      "     0   1.52  20.03 \n",
      "     1  12.65   1.61 \n",
      "\n",
      "   CPU     0      1 \n",
      "     0   2.29   5.86 \n",
      "     1   5.84   2.31 \n",
      "P2P=Enabled Latency (P2P Writes) Matrix (us)\n",
      "   GPU     0      1 \n",
      "     0   1.50   1.27 \n",
      "     1   1.30   1.61 \n",
      "\n",
      "   CPU     0      1 \n",
      "     0   2.31   1.64 \n",
      "     1   1.67   2.33 \n",
      "\n",
      "NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n"
     ]
    }
   ],
   "source": [
    "!cd ../cuda-samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest && make\n",
    "!../cuda-samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
