{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer-to-Peer (P2P) with 2x NVIDIA RTX A5000   \n",
    "\n",
    "- cloud provider: [runpod.io](https://www.runpod.io/)\n",
    "- pod: 2 x RTX A5000 (19 vCPU 100 GB RAM)\n",
    "- image: `runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 27 20:14:25 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A5000               On  | 00000000:56:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              24W / 230W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A5000               On  | 00000000:57:00.0 Off |                  Off |\n",
      "| 30%   28C    P8              15W / 230W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\n",
      "GPU0\t X \tPIX\tSYS\tSYS\t0-23,48-71\t0\t\tN/A\n",
      "GPU1\tPIX\t X \tSYS\tSYS\t0-23,48-71\t0\t\tN/A\n",
      "NIC0\tSYS\tSYS\t X \tPIX\t\t\t\t\n",
      "NIC1\tSYS\tSYS\tPIX\t X \t\t\t\t\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n",
      "\n",
      "NIC Legend:\n",
      "\n",
      "  NIC0: mlx5_0\n",
      "  NIC1: mlx5_1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building [nccl-tests](https://github.com/NVIDIA/nccl-tests/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'nccl-tests' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/nccl-tests.git && cd nccl-tests/ && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running all_reduce_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nThread 1 nGpus 2 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0\n",
      "#\n",
      "# Using devices\n",
      "#  Rank  0 Group  0 Pid 361891 on 58e5f0f169a6 device  0 [0x56] NVIDIA RTX A5000\n",
      "#  Rank  1 Group  0 Pid 361891 on 58e5f0f169a6 device  1 [0x57] NVIDIA RTX A5000\n",
      "#\n",
      "#                                                              out-of-place                       in-place          \n",
      "#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong\n",
      "#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \n",
      "           8             2     float     sum      -1    19.51    0.00    0.00      0    17.81    0.00    0.00      0\n",
      "          16             4     float     sum      -1    18.03    0.00    0.00      0    17.85    0.00    0.00      0\n",
      "          32             8     float     sum      -1    17.63    0.00    0.00      0    17.52    0.00    0.00      0\n",
      "          64            16     float     sum      -1    18.69    0.00    0.00      0    16.03    0.00    0.00      0\n",
      "         128            32     float     sum      -1    17.11    0.01    0.01      0    16.08    0.01    0.01      0\n",
      "         256            64     float     sum      -1    15.28    0.02    0.02      0    15.56    0.02    0.02      0\n",
      "         512           128     float     sum      -1    15.99    0.03    0.03      0    14.54    0.04    0.04      0\n",
      "        1024           256     float     sum      -1    14.88    0.07    0.07      0    14.81    0.07    0.07      0\n",
      "        2048           512     float     sum      -1    13.70    0.15    0.15      0    13.83    0.15    0.15      0\n",
      "        4096          1024     float     sum      -1    14.48    0.28    0.28      0    13.23    0.31    0.31      0\n",
      "        8192          2048     float     sum      -1    13.14    0.62    0.62      0    13.56    0.60    0.60      0\n",
      "       16384          4096     float     sum      -1    13.65    1.20    1.20      0    13.08    1.25    1.25      0\n",
      "       32768          8192     float     sum      -1    14.14    2.32    2.32      0    14.28    2.29    2.29      0\n",
      "       65536         16384     float     sum      -1    13.44    4.87    4.87      0    13.40    4.89    4.89      0\n",
      "      131072         32768     float     sum      -1    33.02    3.97    3.97      0    32.75    4.00    4.00      0\n",
      "      262144         65536     float     sum      -1    39.71    6.60    6.60      0    39.44    6.65    6.65      0\n",
      "      524288        131072     float     sum      -1    49.13   10.67   10.67      0    50.14   10.46   10.46      0\n",
      "     1048576        262144     float     sum      -1    72.46   14.47   14.47      0    72.43   14.48   14.48      0\n",
      "     2097152        524288     float     sum      -1    114.9   18.25   18.25      0    111.8   18.76   18.76      0\n",
      "     4194304       1048576     float     sum      -1    205.9   20.37   20.37      0    205.0   20.46   20.46      0\n",
      "     8388608       2097152     float     sum      -1    394.1   21.29   21.29      0    393.3   21.33   21.33      0\n",
      "    16777216       4194304     float     sum      -1    775.8   21.63   21.63      0    773.4   21.69   21.69      0\n",
      "    33554432       8388608     float     sum      -1   1531.5   21.91   21.91      0   1535.9   21.85   21.85      0\n",
      "    67108864      16777216     float     sum      -1   3050.6   22.00   22.00      0   3048.1   22.02   22.02      0\n",
      "   134217728      33554432     float     sum      -1   6081.3   22.07   22.07      0   6080.1   22.08   22.08      0\n",
      "# Out of bounds values : 0 OK\n",
      "# Avg bus bandwidth    : 7.72402 \n",
      "#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nccl-tests/build/all_reduce_perf -b 8 -e 128M -f 2 -g 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0->cuda:1: 24.572 GB/s\n",
      "cuda:1->cuda:0: 24.535 GB/s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "device0 = torch.device(\"cuda\", 0)\n",
    "device1 = torch.device(\"cuda\", 1)\n",
    "\n",
    "x0 = torch.randn(1024, 1024, 1024, dtype=torch.float32, device=device0)\n",
    "x1 = torch.randint(0,100, (1024, 1024, 1024), dtype=torch.long, device=device1)\n",
    "\n",
    "def copy_tensor(x, dest_device):\n",
    "    y = x.to(dest_device, non_blocking=False, copy=False)\n",
    "    return y\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='copy_tensor(x0, device1)',\n",
    "    setup='from __main__ import copy_tensor',\n",
    "    globals={'x0': x0, 'device1': device1},\n",
    "    num_threads=1)\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='copy_tensor(x1, device1)',\n",
    "    setup='from __main__ import copy_tensor',\n",
    "    globals={'x1': x1, 'device1': device0},\n",
    "    num_threads=1)\n",
    "\n",
    "# sanity check\n",
    "s0 = x0.sum().cpu()\n",
    "y1 = copy_tensor(x0, device1)\n",
    "s1 = y1.sum().cpu()\n",
    "assert torch.abs(s0-s1) < 1e-5\n",
    "\n",
    "m0 = t0.timeit(100)\n",
    "storage_size0 = x0.untyped_storage().size()\n",
    "print(f\"{device0}->{device1}: {storage_size0/m0.mean/2**30:.3f} GB/s\")\n",
    "\n",
    "m1 = t1.timeit(100)\n",
    "storage_size1 = x1.untyped_storage().size()\n",
    "print(f\"{device1}->{device0}: {storage_size1/m1.mean/2**30:.3f} GB/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank=0 send N=50, elapsed_time=8.6825s, 23.035 GB/s, sum=-9481.1240234375 (cuda:0)\n",
      "rank=1 recv N=50, elapsed_time=8.6485s, 23.125 GB/s, sum=-9481.1240234375 (cuda:1)\n",
      "rank=0 broadcast(a, src=0) N=50, elapsed_time=9.5473s, 20.948 GB/s, sum=-9481.1240234375 (cuda:0)\n",
      "rank=0 broadcast(a, src=1) N=50, elapsed_time=9.3507s, 21.389 GB/s, sum=-9481.1240234375 (cuda:0)\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=1 /usr/local/bin/torchrun --standalone --nnodes=1 --nproc-per-node 2 torch_distributed_nccl_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
