{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer-to-Peer (P2P) with 2x NVIDIA RTX 4090\n",
    "\n",
    "- cloud provider: [runpod.io](https://www.runpod.io/)\n",
    "- pod: 2 x RTX 4090 (25 vCPU 200 GB RAM)\n",
    "- image: `runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 28 11:07:05 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0 Off |                  Off |\n",
      "|  0%   23C    P8              15W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:C1:00.0 Off |                  Off |\n",
      "|  0%   23C    P8              19W / 450W |      1MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\n",
      "GPU0\t X \tSYS\t0-63\t0\t\tN/A\n",
      "GPU1\tSYS\t X \t0-63\t0\t\tN/A\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi topo -m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building [nccl-tests](https://github.com/NVIDIA/nccl-tests/tree/master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make -C src build BUILDDIR=/root/p2p-perf/nccl-tests/build\n",
      "make[1]: Entering directory '/root/p2p-perf/nccl-tests/src'\n",
      "Compiling  timer.cc                            > /root/p2p-perf/nccl-tests/build/timer.o\n",
      "Compiling /root/p2p-perf/nccl-tests/build/verifiable/verifiable.o\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling  all_reduce.cu                       > /root/p2p-perf/nccl-tests/build/all_reduce.o\n",
      "Compiling  common.cu                           > /root/p2p-perf/nccl-tests/build/common.o\n",
      "Linking  /root/p2p-perf/nccl-tests/build/all_reduce.o > /root/p2p-perf/nccl-tests/build/all_reduce_perf\n",
      "Compiling  all_gather.cu                       > /root/p2p-perf/nccl-tests/build/all_gather.o\n",
      "Linking  /root/p2p-perf/nccl-tests/build/all_gather.o > /root/p2p-perf/nccl-tests/build/all_gather_perf\n",
      "Compiling  broadcast.cu                        > /root/p2p-perf/nccl-tests/build/broadcast.o\n",
      "Linking  /root/p2p-perf/nccl-tests/build/broadcast.o > /root/p2p-perf/nccl-tests/build/broadcast_perf\n",
      "Compiling  reduce_scatter.cu                   > /root/p2p-perf/nccl-tests/build/reduce_scatter.o\n",
      "Linking  /root/p2p-perf/nccl-tests/build/reduce_scatter.o > /root/p2p-perf/nccl-tests/build/reduce_scatter_perf\n",
      "Compiling  reduce.cu                           > /root/p2p-perf/nccl-tests/build/reduce.o\n",
      "Linking  /root/p2p-perf/nccl-tests/build/reduce.o > /root/p2p-perf/nccl-tests/build/reduce_perf\n",
      "Compiling  alltoall.cu                         > /root/p2p-perf/nccl-tests/build/alltoall.o\n",
      "Linking  /root/p2p-perf/nccl-tests/build/alltoall.o > /root/p2p-perf/nccl-tests/build/alltoall_perf\n",
      "Compiling  scatter.cu                          > /root/p2p-perf/nccl-tests/build/scatter.o\n",
      "Linking  /root/p2p-perf/nccl-tests/build/scatter.o > /root/p2p-perf/nccl-tests/build/scatter_perf\n",
      "Compiling  gather.cu                           > /root/p2p-perf/nccl-tests/build/gather.o\n",
      "Linking  /root/p2p-perf/nccl-tests/build/gather.o > /root/p2p-perf/nccl-tests/build/gather_perf\n",
      "Compiling  sendrecv.cu                         > /root/p2p-perf/nccl-tests/build/sendrecv.o\n",
      "Linking  /root/p2p-perf/nccl-tests/build/sendrecv.o > /root/p2p-perf/nccl-tests/build/sendrecv_perf\n",
      "Compiling  hypercube.cu                        > /root/p2p-perf/nccl-tests/build/hypercube.o\n",
      "Linking  /root/p2p-perf/nccl-tests/build/hypercube.o > /root/p2p-perf/nccl-tests/build/hypercube_perf\n",
      "make[1]: Leaving directory '/root/p2p-perf/nccl-tests/src'\n"
     ]
    }
   ],
   "source": [
    "!cd ../nccl-tests/ && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running all_reduce_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# nThread 1 nGpus 2 minBytes 8 maxBytes 134217728 step: 2(factor) warmup iters: 5 iters: 20 agg iters: 1 validation: 1 graph: 0\n",
      "#\n",
      "# Using devices\n",
      "#  Rank  0 Group  0 Pid   3543 on 5a30cdebc0c3 device  0 [0x01] NVIDIA GeForce RTX 4090\n",
      "#  Rank  1 Group  0 Pid   3543 on 5a30cdebc0c3 device  1 [0xc1] NVIDIA GeForce RTX 4090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "#                                                              out-of-place                       in-place          \n",
      "#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong\n",
      "#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \n",
      "           8             2     float     sum      -1    10.64    0.00    0.00      0    10.94    0.00    0.00      0\n",
      "          16             4     float     sum      -1     9.89    0.00    0.00      0    10.07    0.00    0.00      0\n",
      "          32             8     float     sum      -1     9.81    0.00    0.00      0    10.29    0.00    0.00      0\n",
      "          64            16     float     sum      -1     9.97    0.01    0.01      0     9.78    0.01    0.01      0\n",
      "         128            32     float     sum      -1     9.74    0.01    0.01      0     9.89    0.01    0.01      0\n",
      "         256            64     float     sum      -1     9.99    0.03    0.03      0     9.85    0.03    0.03      0\n",
      "         512           128     float     sum      -1     9.86    0.05    0.05      0     9.98    0.05    0.05      0\n",
      "        1024           256     float     sum      -1     9.85    0.10    0.10      0     9.84    0.10    0.10      0\n",
      "        2048           512     float     sum      -1    11.65    0.18    0.18      0    11.37    0.18    0.18      0\n",
      "        4096          1024     float     sum      -1    11.44    0.36    0.36      0    11.30    0.36    0.36      0\n",
      "        8192          2048     float     sum      -1    11.58    0.71    0.71      0    11.48    0.71    0.71      0\n",
      "       16384          4096     float     sum      -1    15.15    1.08    1.08      0    15.37    1.07    1.07      0\n",
      "       32768          8192     float     sum      -1    23.18    1.41    1.41      0    23.53    1.39    1.39      0\n",
      "       65536         16384     float     sum      -1    40.21    1.63    1.63      0    40.73    1.61    1.61      0\n",
      "      131072         32768     float     sum      -1    56.21    2.33    2.33      0    55.31    2.37    2.37      0\n",
      "      262144         65536     float     sum      -1    95.61    2.74    2.74      0    96.49    2.72    2.72      0\n",
      "      524288        131072     float     sum      -1    159.2    3.29    3.29      0    159.8    3.28    3.28      0\n",
      "     1048576        262144     float     sum      -1    331.6    3.16    3.16      0    332.3    3.16    3.16      0\n",
      "     2097152        524288     float     sum      -1    644.1    3.26    3.26      0    644.9    3.25    3.25      0\n",
      "     4194304       1048576     float     sum      -1   1253.6    3.35    3.35      0   1252.5    3.35    3.35      0\n",
      "     8388608       2097152     float     sum      -1   2497.0    3.36    3.36      0   2497.5    3.36    3.36      0\n",
      "    16777216       4194304     float     sum      -1   4966.1    3.38    3.38      0   4978.7    3.37    3.37      0\n",
      "    33554432       8388608     float     sum      -1   9954.0    3.37    3.37      0   9501.1    3.53    3.53      0\n",
      "    67108864      16777216     float     sum      -1    20342    3.30    3.30      0    20353    3.30    3.30      0\n",
      "   134217728      33554432     float     sum      -1    40971    3.28    3.28      0    40297    3.33    3.33      0\n",
      "# Out of bounds values : 0 OK\n",
      "# Avg bus bandwidth    : 1.6186 \n",
      "#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!../nccl-tests/build/all_reduce_perf -b 8 -e 128M -f 2 -g 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0->cuda:1: 20.412 GB/s\n",
      "cuda:1->cuda:0: 20.614 GB/s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "device0 = torch.device(\"cuda\", 0)\n",
    "device1 = torch.device(\"cuda\", 1)\n",
    "\n",
    "x0 = torch.randn(1024, 1024, 1024, dtype=torch.float32, device=device0)\n",
    "x1 = torch.randint(0,100, (1024, 1024, 1024), dtype=torch.long, device=device1)\n",
    "\n",
    "def copy_tensor(x, dest_device):\n",
    "    y = x.to(dest_device, non_blocking=False, copy=False)\n",
    "    return y\n",
    "\n",
    "t0 = benchmark.Timer(\n",
    "    stmt='copy_tensor(x0, device1)',\n",
    "    setup='from __main__ import copy_tensor',\n",
    "    globals={'x0': x0, 'device1': device1},\n",
    "    num_threads=1)\n",
    "\n",
    "t1 = benchmark.Timer(\n",
    "    stmt='copy_tensor(x1, device1)',\n",
    "    setup='from __main__ import copy_tensor',\n",
    "    globals={'x1': x1, 'device1': device0},\n",
    "    num_threads=1)\n",
    "\n",
    "# sanity check\n",
    "s0 = x0.sum().cpu()\n",
    "y1 = copy_tensor(x0, device1)\n",
    "s1 = y1.sum().cpu()\n",
    "assert torch.abs(s0-s1) < 1e-5\n",
    "\n",
    "m0 = t0.timeit(100)\n",
    "storage_size0 = x0.untyped_storage().size()\n",
    "print(f\"{device0}->{device1}: {storage_size0/m0.mean/2**30:.3f} GB/s\")\n",
    "\n",
    "m1 = t1.timeit(100)\n",
    "storage_size1 = x1.untyped_storage().size()\n",
    "print(f\"{device1}->{device0}: {storage_size1/m1.mean/2**30:.3f} GB/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank=1 recv N=50, elapsed_time=63.7232s, 3.139 GB/s, sum=10155.72265625 (cuda:1)\n",
      "rank=0 send N=50, elapsed_time=63.7232s, 3.139 GB/s, sum=10155.72265625 (cuda:0)\n",
      "rank=0 broadcast(a, src=0) N=50, elapsed_time=64.0530s, 3.122 GB/s, sum=10155.72265625 (cuda:0)\n",
      "rank=0 broadcast(a, src=1) N=50, elapsed_time=51.9199s, 3.852 GB/s, sum=10155.72265625 (cuda:0)\n"
     ]
    }
   ],
   "source": [
    "!OMP_NUM_THREADS=1 torchrun --standalone --nnodes=1 --nproc-per-node 2 torch_distributed_nccl_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0+cu121'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running cuda-samples simpleP2P and p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o simpleP2P.o -c simpleP2P.cu\n",
      "/usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o simpleP2P simpleP2P.o \n",
      "mkdir -p ../../../bin/x86_64/linux/release\n",
      "cp simpleP2P ../../../bin/x86_64/linux/release\n",
      "[../cuda-samples/Samples/0_Introduction/simpleP2P/simpleP2P] - Starting...\n",
      "Checking for multiple GPUs...\n",
      "CUDA-capable device count: 2\n",
      "\n",
      "Checking GPU(s) for support of peer to peer memory access...\n",
      "> Peer access from NVIDIA GeForce RTX 4090 (GPU0) -> NVIDIA GeForce RTX 4090 (GPU1) : No\n",
      "> Peer access from NVIDIA GeForce RTX 4090 (GPU1) -> NVIDIA GeForce RTX 4090 (GPU0) : No\n",
      "Two or more GPUs with Peer-to-Peer access capability are required for ../cuda-samples/Samples/0_Introduction/simpleP2P/simpleP2P.\n",
      "Peer to Peer access is not available amongst GPUs in the system, waiving test.\n"
     ]
    }
   ],
   "source": [
    "!cd ../cuda-samples/Samples/0_Introduction/simpleP2P/ && make\n",
    "!../cuda-samples/Samples/0_Introduction/simpleP2P/simpleP2P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda/bin/nvcc -ccbin g++ -I../../../Common -m64 --threads 0 --std=c++11 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o p2pBandwidthLatencyTest.o -c p2pBandwidthLatencyTest.cu\n",
      "/usr/local/cuda/bin/nvcc -ccbin g++ -m64 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_89,code=sm_89 -gencode arch=compute_90,code=sm_90 -gencode arch=compute_90,code=compute_90 -o p2pBandwidthLatencyTest p2pBandwidthLatencyTest.o \n",
      "mkdir -p ../../../bin/x86_64/linux/release\n",
      "cp p2pBandwidthLatencyTest ../../../bin/x86_64/linux/release\n",
      "[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\n",
      "Device: 0, NVIDIA GeForce RTX 4090, pciBusID: 1, pciDeviceID: 0, pciDomainID:0\n",
      "Device: 1, NVIDIA GeForce RTX 4090, pciBusID: c1, pciDeviceID: 0, pciDomainID:0\n",
      "Device=0 CANNOT Access Peer Device=1\n",
      "Device=1 CANNOT Access Peer Device=0\n",
      "\n",
      "***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\n",
      "So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\n",
      "\n",
      "P2P Connectivity Matrix\n",
      "     D\\D     0     1\n",
      "     0\t     1     0\n",
      "     1\t     0     1\n",
      "Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1 \n",
      "     0 911.06  21.60 \n",
      "     1  22.15 921.83 \n",
      "Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n",
      "   D\\D     0      1 \n",
      "     0 913.74  21.91 \n",
      "     1  22.09 922.37 \n",
      "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1 \n",
      "     0 916.91  30.92 \n",
      "     1  31.25 923.19 \n",
      "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1 \n",
      "     0 918.31  31.19 \n",
      "     1  31.22 923.98 \n",
      "P2P=Disabled Latency Matrix (us)\n",
      "   GPU     0      1 \n",
      "     0   1.39  12.46 \n",
      "     1  17.28   1.45 \n",
      "\n",
      "   CPU     0      1 \n",
      "     0   2.79   8.30 \n",
      "     1   7.97   2.74 \n",
      "P2P=Enabled Latency (P2P Writes) Matrix (us)\n",
      "   GPU     0      1 \n",
      "     0   1.39  12.17 \n",
      "     1  11.72   1.45 \n",
      "\n",
      "   CPU     0      1 \n",
      "     0   2.79   8.10 \n",
      "     1   8.09   2.76 \n",
      "\n",
      "NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n"
     ]
    }
   ],
   "source": [
    "!cd ../cuda-samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest && make\n",
    "!../cuda-samples/Samples/5_Domain_Specific/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
